# Error Control in Exploratory ANOVA
```{r include=FALSE}
load("data/error_data.RData")
library(Superpower)
library(magrittr)
library(kableExtra)
```
In a 2 x 2 x 2 design, an ANOVA will give the test results for three main effects, three two-way interactions, and one three-way interaction. Thatâ€™s 7 statistical tests. The probability of making at least one type I error in a single 2 x 2 x 2 ANOVA is $1-(0.95)^7 = 30$%.

```{r error_control}
string <- "2b*2b*2b"
n <- 50
mu <- c(20, 20, 20, 20, 20, 20, 20, 20) 
# All means are equal - so there is no real difference.
# Enter means in the order that matches the labels below.
sd <- 5
p_adjust = "none"
# "none" means we do not correct for multiple comparisons
labelnames <- c("condition1", "a", "b", 
                "condition2", "c", "d", 
                "condition3", "e", "f") #
# The label names should be in the order of the means specified above.
design_result <- ANOVA_design(design = string,
                   n = n, 
                   mu = mu, 
                   sd = sd, 
                   labelnames = labelnames)
alpha_level <- 0.05
#We set the alpha level at 0.05. 
```
```{r eval=FALSE}
simulation_result <- ANOVA_power(design_result,
                                 alpha_level = alpha_level,
                                 verbose = FALSE)
```

```{r echo=FALSE}
knitr::kable(simulation_result_8.1$main_result,
             caption = "Simulated ANOVA Result") %>%
  kable_styling(latex_options = "hold_position")
```

When there is no true effect, we formally do not have 'power' (which is defined as the probability of finding p < $\alpha$ if there is a true effect to be found) so the power column should be read as the 'type I error rate'. Because we have saved the power simulation in the 'simulation_result' object, we can perform calculations on the 'sim_data' dataframe that is stored. This dataframe contains the results for the nsims simulations (e.g., 10000 rows if you ran 10000 simulations) and stores the p-values and effect size estimates for each ANOVA. The first 7 columns are the p-values for the ANOVA, first the main effects of condition 1, 2, and 3, then three two-way interactions, and finally the threeway interaction.

We can calculate the number of significant results for each test (which should be 5%) by counting the number of significant p-values in each of the 7 rows:

```{r}
apply(as.matrix(simulation_result_8.1$sim_data[(1:7)]), 2, 
    function(x) round(mean(ifelse(x < alpha_level, 1, 0)),4))
```

This is the type I error rate for each test. When we talk about error rate inflation due to multiple comparisons, we are talking about the probability that you conclude there is an effect, when there is actually no effect, when there is a significant effect for the main effect of condition 1, or condition 2, or condition 3, or for the two-way interaction between condition 1 and 2, or condition 1 and 3, or condition 2 and 3, or in the threeway interaction. 

To calculate this error rate we do not just add the 7 error rates (so 7 * 5% - 35%). Instead, we calculate the probability that there will be at least one significant result in an ANOVA we perform. Some ANOVA results will have multiple significant results, just due to the type I error rate (e.g., a significant result for the three-way interaction, and for the main effect of condition 1) but such an ANOVA is counted only once. If we calculate this percentage from our simulations, we see the number is indeed very close to 1-(0.95)^7 = 30%.  

```{r}
sum(apply(as.matrix(simulation_result_8.1$sim_data[(1:7)]), 1, 
    function(x) 
      round(mean(ifelse(x < alpha_level, 1, 0)),4)) > 0) / nsims
```

The question is what we should do about this alpha inflation. It is undesirable if you perform exploratory ANOVA's and are fooled too often by type I errors, which will not replicate if you try to build on them. Therefore, you need to control the type I error rate. 

In the simulation code, which relies on the afex package, there is the option to set p_adjust. In the simulation above, p_adjust was set to "none". This means no adjustment is mage to which p-values are considered to be significant, and the alpha level is used as it is set in the simulation (above this was 0.05).

Afex relies on the `p.adjust` functon in the `stats` package in base R (more information is available [here](https://www.rdocumentation.org/packages/stats/versions/3.1.1/topics/p.adjust)). From the package details: 

>The adjustment methods include the Bonferroni correction ("bonferroni") in which the p-values are multiplied by the number of comparisons. Less conservative corrections are also included by Holm (1979) ("holm"), Hochberg (1988) ("hochberg"), Hommel (1988) ("hommel"), Benjamini & Hochberg (1995) ("BH" or its alias "fdr"), and Benjamini & Yekutieli (2001) ("BY"), respectively. A pass-through option ("none") is also included.  The first four methods are designed to give strong control of the family-wise error rate. There seems no reason to use the unmodified Bonferroni correction because it is dominated by Holm's method, which is also valid under arbitrary assumptions.

>Hochberg's and Hommel's methods are valid when the hypothesis tests are independent or when they are non-negatively associated (Sarkar, 1998; Sarkar and Chang, 1997). Hommel's method is more powerful than Hochberg's, but the difference is usually small and the Hochberg p-values are faster to compute.

>The "BH" (aka "fdr") and "BY" method of Benjamini, Hochberg, and Yekutieli control the false discovery rate, the expected proportion of false discoveries amongst the rejected hypotheses. The false discovery rate is a less stringent condition than the family-wise error rate, so these methods are more powerful than the others.

Let's re-run the simulation twith the Holm-Bonferroni correction, which is simple and require no assumptions.

```{r}
string <- "2b*2b*2b"
n <- 50
mu <- c(20, 20, 20, 20, 20, 20, 20, 20) 
#All means are equal - so there is no real difference.
# Enter means in the order that matches the labels below.
sd <- 5
p_adjust = "holm"
# Changed to Holm-Bonferroni
labelnames <- c("condition1", "a", "b", 
                "condition2", "c", "d", 
                "condition3", "e", "f") #
# the label names should be in the order of the means specified above.
design_result <- ANOVA_design(design = string,
                   n = n, 
                   mu = mu, 
                   sd = sd, 
                   labelnames = labelnames)
alpha_level <- 0.05
```
```{r eval=FALSE}
simulation_result <- ANOVA_power(design_result, 
                                 alpha_level = alpha_level,
                                 p_adjust = p_adjust,
                                 verbose = FALSE)
```

```{r echo = FALSE}
knitr::kable(simulation_result_8.2$main_results, 
             caption = "ANOVA Results")%>%
  kable_styling(latex_options = "hold_position")
```

```{r echo = FALSE}
knitr::kable(simulation_result_8.2$pc_results, 
             caption = "Pairwise Results")%>%
  kable_styling(latex_options = "hold_position")
```

```{r}
sum(apply(as.matrix(simulation_result_8.2$sim_data[(1:7)]), 1, 
    function(x) 
      round(mean(ifelse(x < alpha_level, 1, 0) * 100),4)) > 0) / nsims
```

We see it is close to 5%. 
Note that error rates have variation, and even in a ten thousand simulations, the error rate in the sample of studies can easily be half a percentage point higher or lower (see [this blog](https://daniellakens.blogspot.com/2020/01/observed-alpha-levels-why-statistical.html). 
But *in the long run* the error rate should equal the alpha level. Furthermore, note that the [Holm-Bonferroni](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method) method is slightly more powerful than the Bonferroni procedure (which is simply $\alpha$ divided by the number of tests). 
There are more powerful procedures to control the type I error rate, which require making more assumptions. 
For a small number of tests, they Holm-Bonferroni procedure works well. 
Alternative procedures to control error rates can be found in the [multcomp](https://cran.r-project.org/web/packages/multcomp/index.html) R package [@R-multcomp].  