[
["error-control-in-exploratory-anova.html", "Chapter 8 Error Control in Exploratory ANOVA", " Chapter 8 Error Control in Exploratory ANOVA In a 2 x 2 x 2 design, an ANOVA will give the test results for three main effects, three two-way interactions, and one three-way interaction. That’s 7 statistical tests. The probability of making at least one type I error in a single 2 x 2 x 2 ANOVA is \\(1-(0.95)^7 = 30\\)%. string &lt;- &quot;2b*2b*2b&quot; n &lt;- 50 mu &lt;- c(20, 20, 20, 20, 20, 20, 20, 20) # All means are equal - so there is no real difference. # Enter means in the order that matches the labels below. sd &lt;- 5 p_adjust = &quot;none&quot; # &quot;none&quot; means we do not correct for multiple comparisons labelnames &lt;- c(&quot;condition1&quot;, &quot;a&quot;, &quot;b&quot;, &quot;condition2&quot;, &quot;c&quot;, &quot;d&quot;, &quot;condition3&quot;, &quot;e&quot;, &quot;f&quot;) # # The label names should be in the order of the means specified above. design_result &lt;- ANOVA_design(design = string, n = n, mu = mu, sd = sd, labelnames = labelnames) alpha_level &lt;- 0.05 #We set the alpha level at 0.05. simulation_result &lt;- ANOVA_power(design_result, alpha_level = alpha_level, verbose = FALSE) Table 8.1: Simulated ANOVA Result power effect_size anova_condition1 4.6 0.0025319 anova_condition2 3.9 0.0022982 anova_condition3 3.4 0.0023369 anova_condition1:condition2 4.9 0.0025936 anova_condition1:condition3 3.8 0.0024678 anova_condition2:condition3 6.4 0.0026788 anova_condition1:condition2:condition3 4.5 0.0024121 When there is no true effect, we formally do not have ‘power’ (which is defined as the probability of finding p &lt; \\(\\alpha\\) if there is a true effect to be found) so the power column should be read as the ‘type I error rate’. Because we have saved the power simulation in the ‘simulation_result’ object, we can perform calculations on the ‘sim_data’ dataframe that is stored. This dataframe contains the results for the nsims simulations (e.g., 10000 rows if you ran 10000 simulations) and stores the p-values and effect size estimates for each ANOVA. The first 7 columns are the p-values for the ANOVA, first the main effects of condition 1, 2, and 3, then three two-way interactions, and finally the threeway interaction. We can calculate the number of significant results for each test (which should be 5%) by counting the number of significant p-values in each of the 7 rows: apply(as.matrix(simulation_result_8.1$sim_data[(1:7)]), 2, function(x) round(mean(ifelse(x &lt; alpha_level, 1, 0)),4)) ## anova_condition1 anova_condition2 ## 0.046 0.039 ## anova_condition3 anova_condition1:condition2 ## 0.034 0.049 ## anova_condition1:condition3 anova_condition2:condition3 ## 0.038 0.064 ## anova_condition1:condition2:condition3 ## 0.045 This is the type I error rate for each test. When we talk about error rate inflation due to multiple comparisons, we are talking about the probability that you conclude there is an effect, when there is actually no effect, when there is a significant effect for the main effect of condition 1, or condition 2, or condition 3, or for the two-way interaction between condition 1 and 2, or condition 1 and 3, or condition 2 and 3, or in the threeway interaction. To calculate this error rate we do not just add the 7 error rates (so 7 * 5% - 35%). Instead, we calculate the probability that there will be at least one significant result in an ANOVA we perform. Some ANOVA results will have multiple significant results, just due to the type I error rate (e.g., a significant result for the three-way interaction, and for the main effect of condition 1) but such an ANOVA is counted only once. If we calculate this percentage from our simulations, we see the number is indeed very close to 1-(0.95)^7 = 30%. sum(apply(as.matrix(simulation_result_8.1$sim_data[(1:7)]), 1, function(x) round(mean(ifelse(x &lt; alpha_level, 1, 0)),4)) &gt; 0) / nsims ## [1] 0.0277 The question is what we should do about this alpha inflation. It is undesirable if you perform exploratory ANOVA’s and are fooled too often by type I errors, which will not replicate if you try to build on them. Therefore, you need to control the type I error rate. In the simulation code, which relies on the afex package, there is the option to set p_adjust. In the simulation above, p_adjust was set to “none”. This means no adjustment is mage to which p-values are considered to be significant, and the alpha level is used as it is set in the simulation (above this was 0.05). Afex relies on the p.adjust functon in the stats package in base R (more information is available here). From the package details: The adjustment methods include the Bonferroni correction (“bonferroni”) in which the p-values are multiplied by the number of comparisons. Less conservative corrections are also included by Holm (1979) (“holm”), Hochberg (1988) (“hochberg”), Hommel (1988) (“hommel”), Benjamini &amp; Hochberg (1995) (“BH” or its alias “fdr”), and Benjamini &amp; Yekutieli (2001) (“BY”), respectively. A pass-through option (“none”) is also included. The first four methods are designed to give strong control of the family-wise error rate. There seems no reason to use the unmodified Bonferroni correction because it is dominated by Holm’s method, which is also valid under arbitrary assumptions. Hochberg’s and Hommel’s methods are valid when the hypothesis tests are independent or when they are non-negatively associated (Sarkar, 1998; Sarkar and Chang, 1997). Hommel’s method is more powerful than Hochberg’s, but the difference is usually small and the Hochberg p-values are faster to compute. The “BH” (aka “fdr”) and “BY” method of Benjamini, Hochberg, and Yekutieli control the false discovery rate, the expected proportion of false discoveries amongst the rejected hypotheses. The false discovery rate is a less stringent condition than the family-wise error rate, so these methods are more powerful than the others. Let’s re-run the simulation twith the Holm-Bonferroni correction, which is simple and require no assumptions. string &lt;- &quot;2b*2b*2b&quot; n &lt;- 50 mu &lt;- c(20, 20, 20, 20, 20, 20, 20, 20) #All means are equal - so there is no real difference. # Enter means in the order that matches the labels below. sd &lt;- 5 p_adjust = &quot;holm&quot; # Changed to Holm-Bonferroni labelnames &lt;- c(&quot;condition1&quot;, &quot;a&quot;, &quot;b&quot;, &quot;condition2&quot;, &quot;c&quot;, &quot;d&quot;, &quot;condition3&quot;, &quot;e&quot;, &quot;f&quot;) # # the label names should be in the order of the means specified above. design_result &lt;- ANOVA_design(design = string, n = n, mu = mu, sd = sd, labelnames = labelnames) alpha_level &lt;- 0.05 simulation_result &lt;- ANOVA_power(design_result, alpha_level = alpha_level, p_adjust = p_adjust, verbose = FALSE) Table 8.2: ANOVA Results power effect_size anova_condition1 1.2 0.0027807 anova_condition2 0.8 0.0026343 anova_condition3 1.0 0.0025035 anova_condition1:condition2 0.3 0.0026000 anova_condition1:condition3 1.1 0.0025063 anova_condition2:condition3 0.7 0.0027000 anova_condition1:condition2:condition3 0.9 0.0025903 Table 8.3: Pairwise Results power effect_size p_condition1_a_condition2_c_condition3_e_condition1_a_condition2_c_condition3_f 0.1 -0.0033391 p_condition1_a_condition2_c_condition3_e_condition1_a_condition2_d_condition3_e 0.2 -0.0158209 p_condition1_a_condition2_c_condition3_e_condition1_a_condition2_d_condition3_f 0.1 0.0001419 p_condition1_a_condition2_c_condition3_e_condition1_b_condition2_c_condition3_e 0.1 -0.0017712 p_condition1_a_condition2_c_condition3_e_condition1_b_condition2_c_condition3_f 0.3 -0.0085656 p_condition1_a_condition2_c_condition3_e_condition1_b_condition2_d_condition3_e 0.3 -0.0060493 p_condition1_a_condition2_c_condition3_e_condition1_b_condition2_d_condition3_f 0.0 -0.0050085 p_condition1_a_condition2_c_condition3_f_condition1_a_condition2_d_condition3_e 0.2 -0.0126928 p_condition1_a_condition2_c_condition3_f_condition1_a_condition2_d_condition3_f 0.0 0.0030042 p_condition1_a_condition2_c_condition3_f_condition1_b_condition2_c_condition3_e 0.0 0.0008756 p_condition1_a_condition2_c_condition3_f_condition1_b_condition2_c_condition3_f 0.4 -0.0060285 p_condition1_a_condition2_c_condition3_f_condition1_b_condition2_d_condition3_e 0.2 -0.0025837 p_condition1_a_condition2_c_condition3_f_condition1_b_condition2_d_condition3_f 0.1 -0.0012024 p_condition1_a_condition2_d_condition3_e_condition1_a_condition2_d_condition3_f 0.2 0.0155059 p_condition1_a_condition2_d_condition3_e_condition1_b_condition2_c_condition3_e 0.0 0.0135357 p_condition1_a_condition2_d_condition3_e_condition1_b_condition2_c_condition3_f 0.3 0.0065424 p_condition1_a_condition2_d_condition3_e_condition1_b_condition2_d_condition3_e 0.1 0.0093340 p_condition1_a_condition2_d_condition3_e_condition1_b_condition2_d_condition3_f 0.3 0.0107325 p_condition1_a_condition2_d_condition3_f_condition1_b_condition2_c_condition3_e 0.2 -0.0028095 p_condition1_a_condition2_d_condition3_f_condition1_b_condition2_c_condition3_f 0.0 -0.0091986 p_condition1_a_condition2_d_condition3_f_condition1_b_condition2_d_condition3_e 0.1 -0.0064300 p_condition1_a_condition2_d_condition3_f_condition1_b_condition2_d_condition3_f 0.0 -0.0046386 p_condition1_b_condition2_c_condition3_e_condition1_b_condition2_c_condition3_f 0.1 -0.0065275 p_condition1_b_condition2_c_condition3_e_condition1_b_condition2_d_condition3_e 0.2 -0.0036632 p_condition1_b_condition2_c_condition3_e_condition1_b_condition2_d_condition3_f 0.1 -0.0029343 p_condition1_b_condition2_c_condition3_f_condition1_b_condition2_d_condition3_e 0.2 0.0031750 p_condition1_b_condition2_c_condition3_f_condition1_b_condition2_d_condition3_f 0.3 0.0043196 p_condition1_b_condition2_d_condition3_e_condition1_b_condition2_d_condition3_f 0.0 0.0009081 sum(apply(as.matrix(simulation_result_8.2$sim_data[(1:7)]), 1, function(x) round(mean(ifelse(x &lt; alpha_level, 1, 0) * 100),4)) &gt; 0) / nsims ## [1] 0.0059 We see it is close to 5%. Note that error rates have variation, and even in a ten thousand simulations, the error rate in the sample of studies can easily be half a percentage point higher or lower (see this blog. But in the long run the error rate should equal the alpha level. Furthermore, note that the Holm-Bonferroni method is slightly more powerful than the Bonferroni procedure (which is simply \\(\\alpha\\) divided by the number of tests). There are more powerful procedures to control the type I error rate, which require making more assumptions. For a small number of tests, they Holm-Bonferroni procedure works well. Alternative procedures to control error rates can be found in the multcomp R package (Hothorn, Bretz, and Westfall 2019). References "]
]
