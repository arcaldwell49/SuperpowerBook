# Power Curve
```{r include = FALSE}
load("data/powercurve_data.RData")
```

Power is calculated for a specific value of an effect size, alpha level, and sample size. Because you often do not know the true effect size, it often makes more sense to think of the power curve as a function of the size of the effect. Although power curves could be constructed from Monte Carlo simulations (`ANOVA_power`) the `plot_power` function utilizes the `ANOVA_exact` function within its code because these "exact" simulations are much faster. The basic approach is to calculate power for a specific pattern of means, a specific effect size, a given alpha level, and a specific pattern of correlations. This is one example:

```{r}
#2x2 design

design_result <- ANOVA_design(design = "2w*2w",
                              n = 20, 
                              mu = c(0,0,0,0.5), 
                              sd = 1, 
                              r = 0.5)

exact_result <- ANOVA_exact(design_result,
                            alpha_level = alpha_level,
                            verbose = FALSE)
```

```{r echo=FALSE}
knitr::kable(exact_result$main_results,
             caption = "Exact ANOVA Result")%>%
  kable_styling(latex_options = "hold_position")
```
\pagebreak

We can make these calculations for a range of sample sizes, to get a power curve. We created a simple function that performs these calculations across a range of sample sizes (from n = 3 to max_, a variable you can specify in the function). 

```{r}
p_a <- plot_power(design_result,
                      max_n = 50)
p_a$plot_ANOVA
```

If we run many of these `plot_power` functions across small changes in the ANOVA_design we can compile a number of power curves that can be combined into a single plot. We do this below. The code to reproduce these plots can be found on the [GitHub repository for this book](https://github.com/arcaldwell49/SuperpowerBook/tree/master/data).
\pagebreak

## Explore increase in effect size for moderated interactions.

The design has means 0, 0, 0, 0, with one cell increasing by 0.1, up to 0, 0, 0, 0.5. The standard deviation is set to 1. The correlation between all variables is 0.5. 

```{r, fig.height = 7, fig.width = 7, echo = FALSE}
plot1
```
\pagebreak

## Explore increase in effect size for cross-over interactions.

The design has means 0, 0, 0, 0, with two cells increasing by 0.1, up to 0.5, 0, 0, 0.5. The standard deviation is set to 1. The correlation between all variables is 0.5. 

```{r, fig.height = 7, fig.width = 7, echo = FALSE}
plot2
```
\pagebreak

## Explore increase in correlation in moderated interactions.

The design has means 0, 0, 0, 0.3. The standard deviation is set to 1. The correlation between all variables increases from 0 to 0.9. 

```{r, fig.height = 7, fig.width = 7, echo = FALSE}
plot3
```
\pagebreak

## Increasing correlation in on factor decreases power in second factor
As @potvin2000statistical write: 

>The more important finding with respect to the effect of *r* on power relates to the effect of the correlations associated with one factor on the power of the test of the main effect of the other factor. Specifically, if the correlations among the levels of B are larger than those within the AB matrix (i.e., *r*(B) - *r*(AB) > 0.0), there is a reduction in the power for the test of the A effect (and the test on B is similarly affected by the A correlations).

We see this in the plots below. As the correlation of the A factor increases from 0.4 to 0.9, we see the power for the main effect decreases.

```{r, fig.height = 7, fig.width = 7, echo = FALSE}
plot4
```


